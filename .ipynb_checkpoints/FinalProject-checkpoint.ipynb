{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project in Data Science\n",
    "## Fake news Predictor\n",
    "\n",
    "### Group 8\n",
    "*Lykke Laura Sørensen (ltm712)* <br>\n",
    "*Jeppe Ram Pedersen (lxd520)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the relevant libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re  # to be able to clean the text using Regular Expression\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import PorterStemmer\n",
    "!pip install sqlalchemy\n",
    "import sqlalchemy\n",
    "\n",
    "# reading the csv-file\n",
    "# define relevant path for file\n",
    "dataForMilestone2 = '../250t-raw.csv'\n",
    "\n",
    "# read the csv-file and load into dataframe\n",
    "dataRaw = pd.read_csv(dataForMilestone2)\n",
    "\n",
    "data = dataRaw.copy() # So we still have easily access to original data when working on the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make a preliminary investigation of our data, in order to filter out irrelevant tables. We also start by clean-up and structuring of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspecting the data\n",
    "print(data.content,\"\\n\")\n",
    "print(data.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fieldLengthPrinter (string):\n",
    "  print(column, len(data[string].value_counts()))\n",
    "\n",
    "for column in data:\n",
    "  fieldLengthPrinter (column)\n",
    "\n",
    "display(data[:3])\n",
    "\n",
    "def fieldPrinter (string):\n",
    "    print(column, len(data[string]) - data[string].isna().sum())\n",
    "    \n",
    "for column in data:\n",
    "    fieldPrinter(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[\"meta_keywords\"].count()-(data[\"meta_keywords\"].str.len()==4.0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: More date formats needs to be recognized by the regex\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = text.lower() # lowercase\n",
    "    text = re.sub(\"(\\n+|\\t+|\\s{2,})\", \" \", text) # removing multible white spaces, tabs or new lines\n",
    "    text = re.sub(\"[a-z]+ [0-9]{1,2}, [0-9]{4}\", \"<DATE>\", text) # removes date in the format \"month[letters] date, year\"\n",
    "    text = re.sub(\"(https?\\:\\/\\/)?\\w*.\\w*.(com|net)[^\\s\\]]*\", \"<URL>\", text) # urls replaced by <URL>\n",
    "    text = re.sub(\"[0-9]+\", \"<NUM>\", text) # numbers replaced by <NUM>, dates replaces by <DATE>\n",
    "    text = re.sub(\"(\\w+@\\w+.com)\", \"<EMAIL>\", text) # E-mail replaced by <EMAIL>\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DETTE TAGER LANG TID!!!!! KØR DEN KUN HVIS CLEANING AF KOLONNEN \"CONTENT\" SKAL BRUGES. \n",
    "\n",
    "data[\"content\"] = data.apply(lambda row : clean_text(row[\"content\"]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[\"meta_keywords\"][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"meta_keywords\"] = data.apply(lambda row : clean_text(row[\"meta_keywords\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_row', None):\n",
    "  display(data[\"meta_keywords\"][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denne arbejder jeg stadig på\n",
    "\n",
    "# Stemming\n",
    "ps = PorterStemmer()\n",
    "\n",
    "\n",
    "def stem_text(string):\n",
    "    stem = []\n",
    "    for w in data[\"meta_keywords\"]:\n",
    "        stem.append(ps.stem(w))\n",
    "    return stem\n",
    "\n",
    "data[\"meta_keywords\"] = data.apply(lambda row : stem_text(row[\"meta_keywords\"]), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_row', None):\n",
    "  display(data[\"meta_keywords\"][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing purposes\n",
    "relString = 6\n",
    "print(dataRaw[\"content\"][relString],\"\\n\")\n",
    "\n",
    "print(data[\"content\"][relString])\n",
    "\n",
    "#print(clean_text(dataRaw[\"content\"][relString]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to Process the text we will use the nltk library (https://www.nltk.org/)\n",
    "# we have chosen to use our own data_clean_homemade, so we are able to remove dates etc. \n",
    "\n",
    "\n",
    "# tokens have only been tokenized on one content!\n",
    "\n",
    "tokens = nltk.word_tokenize(data[\"content\"][relString]) # to tokenize the text\n",
    "\n",
    "print(\"Number of tokens in text: \", len(tokens))\n",
    "#print(tokens[:10])\n",
    "\n",
    "# stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "# removal of stopwords in tokens\n",
    "tokens_without_stopWords = [w for w in tokens if not w in stopWords]\n",
    "print(\"Number of tokens without stopwords: \", len(tokens_without_stopWords))\n",
    "\n",
    "#print(tokens_without_Stopwords\n",
    "reduction_rate = (len(tokens)-len(tokens_without_stopWords))/len(tokens)\n",
    "\n",
    "print(\"We have removed %d stopwords from the text\" %(len(tokens)-len(tokens_without_stopWords)))\n",
    "print(\"Reduction rate: \", reduction_rate)\n",
    "\n",
    "# Stemming\n",
    "ps = PorterStemmer()\n",
    "\n",
    "stem = []\n",
    "for w in tokens_without_stopWords:\n",
    "  stem.append(ps.stem(w))\n",
    "print(\"Number of tokens in text without stopwords and after stemming: \", len(stem))\n",
    "\n",
    "print(\"\\nTo illustrate the stemming of the words\")\n",
    "print(tokens_without_stopWords[10:20])\n",
    "print(stem[10:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denne arbejder jeg stadig på\n",
    "\n",
    "tokens1 = nltk.word_tokenize(data[\"meta_keywords\"][relString]) # to tokenize the text\n",
    "\n",
    "def tokens_text(string):\n",
    "    tokens = nltk.word_tokenize(string)\n",
    "    return tokens \n",
    "\n",
    "data[\"meta_keywords\"] = data.apply(lambda row : tokens_text(row[\"meta_keywords\"]), axis=1)\n",
    "\n",
    "print(\"Number of tokens in meta_keywords: \", len(tokens))\n",
    "print(data[\"meta_keywords\"][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[\"meta_keywords\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tokens_without_Stopwords\n",
    "#reduction_rate = (len(tokens)-len(tokens_without_stopWords))/len(tokens)\n",
    "\n",
    "#print(\"We have removed %d stopwords from the text\" %(len(tokens)-len(tokens_without_stopWords)))\n",
    "#print(\"Reduction rate: \", reduction_rate)\n",
    "\n",
    "# Stemming\n",
    "#ps = PorterStemmer()\n",
    "\n",
    "#stem = []\n",
    "#for w in tokens_without_stopWords:\n",
    "#  stem.append(ps.stem(w))\n",
    "#print(\"Number of tokens in text without stopwords and after stemming: \", len(stem))\n",
    "\n",
    "#print(\"\\nTo illustrate the stemming of the words\")\n",
    "#print(tokens_without_stopWords[10:20])\n",
    "#print(stem[10:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the database from Milestone 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified dataframe used as source for other dataframes\n",
    "dataMod = data\n",
    "dataMod = dataMod[['id','domain','type','url','content','scraped_at','inserted_at','updated_at','title','authors','meta_keywords','meta_description','tags']]\n",
    "dataMod = dataMod.rename(columns={'id':'ArticleID','domain':'DomainName','type': 'TypeName','authors':'AuthorGroupMembers'})\n",
    "display(dataMod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe for main table\n",
    "dataArticles = dataMod[['ArticleID','url','content','title']]\n",
    "\n",
    "# Dataframes for auxillary tables\n",
    "dataAuthors = dataMod[['AuthorGroupMembers']].drop_duplicates().reset_index(drop=True)\n",
    "dataAuthors['AuthorGroupID'] = dataAuthors.index\n",
    "\n",
    "dataType = dataMod[['TypeName']].drop_duplicates().reset_index(drop=True)\n",
    "dataType['TypeID'] = dataType.index\n",
    "\n",
    "dataDomain = (dataMod[['DomainName']].drop_duplicates().reset_index(drop=True))\n",
    "dataDomain['DomainID'] = dataDomain.index\n",
    "\n",
    "# Dataframes for connecting tables\n",
    "modRawAut = pd.merge(dataAuthors, dataMod, on=['AuthorGroupMembers'], how='outer')\n",
    "ArtIDToAutID = modRawAut[['ArticleID','AuthorGroupID']]\n",
    "\n",
    "modRawType = pd.merge(dataType, dataMod, on=['TypeName'], how='outer')\n",
    "ArtIDToTypeID = modRawType[['ArticleID','TypeID']]\n",
    "\n",
    "modRawDomain = pd.merge(dataDomain, dataMod, on=['DomainName'], how='outer')\n",
    "ArtIDToDomainID = modRawDomain[['ArticleID','DomainID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Dataframes into tables accessible with SQL\n",
    "engine = sqlalchemy.create_engine('sqlite://', echo=False)\n",
    "\n",
    "dataMod.to_sql('mod', con=engine, index=False)\n",
    "dataArticles.to_sql('Articles', con=engine, index=False)\n",
    "dataAuthors.to_sql('AuthorGroups', con=engine, index=False)\n",
    "dataType.to_sql('Types', con=engine, index=False)\n",
    "dataDomain.to_sql('Domains', con=engine, index=False)\n",
    "ArtIDToAutID.to_sql('Art2Aut', con=engine, index=False)\n",
    "ArtIDToTypeID.to_sql('Art2Type', con=engine, index=False)\n",
    "ArtIDToDomainID.to_sql('Art2Dom', con=engine, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(engine.execute(\"SELECT * FROM Art2Dom WHERE DomainID=2\").fetchall())\n",
    "display(engine.execute(\"SELECT url FROM Articles\").fetchall())\n",
    "display(engine.execute(\"SELECT * FROM AuthorGroups WHERE AuthorGroupMembers IS NULL\").fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Task 3.1: List of the domains of the reliable articles scraped after 2018-01-15:\")\n",
    "engine.execute(\"SELECT DISTINCT DomainName FROM mod WHERE TypeName='reliable' AND scraped_at>'2018-01-15'\").fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(\"SELECT DISTINCT AuthorGroupMembers, COUNT(AuthorGroupMembers) AS AuthorCount FROM mod WHERE TypeName='fake' GROUP BY AuthorGroupMembers HAVING AuthorCount>=AuthorCount ORDER BY AuthorCount DESC\").fetchall()[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task3 = engine.execute(\"SELECT meta_keywords FROM mod WHERE LENGTH(meta_keywords)>4 ORDER BY meta_keywords ASC\")\n",
    "task3.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interesting observations\n",
    "numOfDomains = engine.execute(\"SELECT COUNT(DomainID) FROM Domains WHERE DomainName IS NOT NULL\").fetchall()[0][0]\n",
    "print(\"There are %d domains\" % numOfDomains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numOfDomainTypeCombinations = engine.execute(\"SELECT COUNT(*) FROM (SELECT DISTINCT DomainID, TypeID FROM Art2Dom NATURAL JOIN Art2Type)\").fetchall()[0][0]\n",
    "print(\"There are %d domains and %d domain-type combinations\" %(numOfDomains,numOfDomainTypeCombinations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is clear to see that the dataset is biased since every domain only has one type of news, except in one case where \n",
    "# a domain has two types. \n",
    "\n",
    "numOfAuthorGroups = engine.execute(\"SELECT COUNT(AuthorGroupID) FROM AuthorGroups WHERE AuthorGroupMembers IS NOT NULL\").fetchall()[0][0]\n",
    "\n",
    "numOfDomainAuthorsCombinations = engine.execute(\"SELECT COUNT(*) FROM (SELECT DISTINCT DomainID, AuthorGroupID FROM Art2Dom NATURAL JOIN Art2Aut)\").fetchall()[0][0]\n",
    "\n",
    "print(\"There are %d Author groups and %d domain-Author group combinations\" % (numOfAuthorGroups,numOfDomainAuthorsCombinations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is possible that every group of authors only write for a limited number of domains where at least 28495 write for \n",
    "# a single domain. (Due to the current state of the database however, this will be difficult to ascertain since the \n",
    "# table AuthorGroups is not NF1 compliant.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the final project, we will work toward actually creating a Fake News predictor. This will build on the work you have done in the Milestones, combined with the topics covered in the lectures in this second block of the course. \n",
    "\n",
    "### 1. Know your data (~1 page)\n",
    "Your milestones were primarily about getting to know your data and representing it in a reasonable way. The first part of your final project is to summarize the main findings from this process (possibly incorporating feedback that you got in Peergrade):\n",
    "\n",
    "Describe how you ended up representing the FakeNewsCorpus dataset (for instance by describing your ER diagram). Argue for why you chose this design.\n",
    "Did you discover any inherent problems with the data while working with it?\n",
    "Report key properties of the data set - for instance through statistics or visualization. If you use non-trivial SQL queries to extract these properties, please describe them.\n",
    "What were your experiences with scraping your assigned fragment of the \"Politics and Conflict\" section of the Wikinews site?\n",
    "To go further on the work you started with the milestones, we ask you take the following steps:\n",
    "\n",
    "**Create a relational database schema to represent the dataset you scraped from the \"Politics and Conflict\" section of the Wikinews site and import the data you scraped into this schema. Document your schema design in an ER diagram and briefly discuss how you dealt with the metadata in this source.\n",
    "Use SQL to report basic statistics on this additional data source, e.g., number of articles or distribution over dates. \n",
    "Now that you have two different sources in the database, corresponding to the FakeNewsCorpus and to the Wikinews fragment you scraped, create a view that integrates the article information from the two sources. How do you map the different metadata from the sources into a common schema? NOTE: You need at a minimum to create a view schema that will suffice for the modeling task below, though you may include more metadata in the view if possible.\n",
    "Finally, conclude by specifying how you will use the data to train a Fake News predictor:**\n",
    "\n",
    "**Specify which data you will be using to train and test the models in the remaining part of this project. Does it makes sense to include the Wikinews data or will you limit yourself to (a subset of) FakeNewsCorpus. Argue why.\n",
    "In this project, we will consider fake news detection as a binary classification problem. Find a good way to aggregate the many output classes of FakeNewsCorpus into 2 classes (FAKE/REAL). Argue why this is a reasonable choice.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Establish a baseline (~0.5 page)\n",
    "The next step is to create one or more reasonable baselines for your Fake News predictor. These should be simple models that you can use to benchmark your more advanced models against later.\n",
    "\n",
    "Start by considering only features extracted from the main text (content) field. Choose one or more simple baseline models, train them, and report their accuracies. Also remember to report any necessary details about your baseline models (e.g., the choice of relevant parameters and how you chose them). Describe why you chose these baseline models - why are they reasonable?\n",
    "Consider whether it would make sense to include meta-data features as well. If so, which ones, and why? If relevant, report the performance when including these additional features and compare it to the first baselines. Discuss whether these results match your expectations.\n",
    "For the remainder of the project, we will limit ourselves to main-text data only (i.e. no meta-data). This makes it easier to do the cross-domain experiment in question 4 (which does not have the same set of meta-data fields)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create a Fake News predictor (~1 page)\n",
    "Create the best Fake News predictor that you can come up with. This should be a more complex model than the one(s) you used as baseline, either in the sense that it uses a more advanced method, or because it uses a more elaborate set of features. Report necessary details about your models ensuring full reproducibility. This could include, for example, the choice of relevant parameters and how you chose them. \n",
    "\n",
    "Quantify the performance of your Fake News predictor against your baseline(s).\n",
    "Argue for why you chose this approach over potential alternatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Performance beyond the original dataset (~0.5 page)\n",
    "Now, we will test how well the model works beyond the dataset that you described in question 1.\n",
    "\n",
    "We have set up a friendly competition between the groups. The idea is that we provide a dataset *without* labels (CSV format, two columns: ID,text), and that you all use your model to try to predict the labels. You will then upload a file with the ID and the labels (CSV format: two columns: ID, \"REAL\" or \"FAKE\"). We will then compare your predictions against the true labels and create an online leaderboard where you can see your rank compared to the other groups. The leaderboard is hosted as a Kaggle competition accessible here (Links to an external site.) (Links to an external site.). You can also find the test data set there. Please don't try to reverse-engineer the source of the data we provide, in order to download and train on it (we will be able to tell).\n",
    "In order to allow you to play around cross-domain performance locally as well, try the same exercise on the LIAR dataset (https://www.cs.ucsb.edu/~william/data/liar_dataset.zip (Links to an external site.)), where you know the labels, and can thus immediately calculate the performance.\n",
    "Compare the results of these two experiments to the results you obtained in question 3. Report both your LIAR results and the leaderboard results as part of your report. Remember to test the performance of your baseline model as well.\n",
    "Arrange all these results in a table to facilitate a comparison between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Discussion (~1 page)\n",
    "Conclude your report by discussing the results you obtained.\n",
    "\n",
    "Explain the discrepancy between the performance on your test set and on the LIAR set and leaderboard. If relevant, use visualizations or report relevant statistics to point out differences in the datasets.\n",
    "Conclude with describing overall lessons learned during the project, for instance considering questions like: Does the discrepancy between performance on different data sets surprise you? What can be done to improve the performance of Fake News prediction? Will further progress be driven primarily by better models or by better data? Is it even a solvable problem?\n",
    "Please note that this question is not merely a summary of what you have done in the other questions. We expect to see some non-trivial reflection in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He belives that we should know what neural networks is, so we can apply it for our final project. We can use it by Embeddings. (See the end of video 3, Lecture on Neural networks in Week 18, Thursday). Vi behøver ikke at lave word embedding selv. Der findes pre-trained modeller, som vi bare kan hente ned. \n",
    "\n",
    "Feedback: Vi kan sammenligne 'content'. \n",
    "\n",
    "We have to vectorize the sentences (by tokenizing it). We can use TFIDF.\n",
    "\n",
    "If we use the BERT tokenizer, we will have to explain how it works in the report. \n",
    "\n",
    "Vi kan bruge PCA eller cluster til analysen for at se, hvordan de forskellgige tags er. Ellers kan vi kigge på beskrivelsen. HVsi vi kun bruger FAKE/REAL, så er det måske lidt naivt. \n",
    "Cluster er ikke nok, så vi skal også bruge Classification (ligesom vi gjorde til øvelsen 2021.05.27)\n",
    "Encoding er vigtigt. \n",
    "\n",
    "Jupyter notebook fra øvelsen 2021.06.03 har de \"næsten\" lavet slutningen på vores opgave."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
